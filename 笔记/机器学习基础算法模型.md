# 机器学习基础算法模型

## 1.knn算法（k-Nearest Neighbors）-k近邻算法

### k近邻算法中使用到的几个距离：欧拉距离、曼哈顿距离、明科夫斯基距离，经过推导，本质都是明科夫斯基距离

![](Snipaste_2020-11-12_09-11-46.png)

式中的a和b分别代表第a个和第b个样本，1,2,3,....,n代表a/b样本中的第1,2,3,...,n个维度

![](Snipaste_2020-11-12_17-02-40.png)

![](Snipaste_2020-11-12_17-06-36.png)



### knn算法的基本过程

```python
import numpy as np
import matplotlib.pyplot as plt

raw_data_X = [[3.393533211, 2.331273381],
              [3.110073483, 1.781539638],
              [1.343808831, 3.368360954],
              [3.582294042, 4.679179110],
              [2.280362439, 2.866990263],
              [7.423436942, 4.696522875],
              [5.745051997, 3.533989803],
              [9.172168622, 2.511101045],
              [7.792783481, 3.424088941],
              [7.939820817, 0.791637231]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)
#待预测的x
x = np.array([8.093607318, 3.365731514])
k = 6
from math import sqrt
#首先要找出与x距离最近的6个点，此处使用欧拉距离，因此要计算distance
distances = []
for x_train in X_train:
    distances.append(sqrt(np.sum((x_train - x) ** 2)))
#筛选出k个最小的,因此需要排序
#排序好得到索引
nearest = np.argsort(distances)
#找到k个与x最近的样本的类别（即y值）
topK_y = [y_train[i] for i in nearest[:k]]
#分类投票，看哪个类别占得比重大
from collections import Counter
votes = Counter(topK_y)
print(votes)
#得到预测值predict_y
predict_y = votes.most_common(1)[0][0]
```

### 使用scikit-learn中的knn算法

```python
from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier(n_neighbors=6)
knn_classifier.fit(X_train,y_train)
x= x.reshape(1,-1)
y_predict = knn_classifier.predict(x)
print(y_predict)
```

### 模拟封装scikit-learn中的knn算法

```python
class KNNClassifier:
    def __init__(self,k):
        assert k >= 1,"k must be valid"
        self.k = k
        self.__X_train = None
        self.__y_train = None
    def fit(self,X_train,y_train):
        assert self.k <= X_train.shape[0], "k must be valid"
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        """根据训练数据集"X_train和y_train"训练KNN分类器"""
        self.__X_train = X_train
        self.__y_train = y_train
        return self
    def predict(self,X_predict):
        assert self.__X_train is not None and self.__y_train is not None,\
        "must fit before predict"
        assert X_predict.shape[1] == self.__X_train.shape[1],\
        "the feature number of X_predict must be equal to X_train"
        y_predict = [self._predict(x) for x in X_predict]
        return np.array(y_predict)
    def _predict(self,x):
        assert x.shape[0] == self.__X_train.shape[1],\
        "the feature number of x must be equal to X_train"
        # 计算距离
        distances = [sqrt(np.sum((x_train - x) ** 2)) for x_train in self.__X_train]
        # 排序
        nearest = np.argsort(distances)
        topK_y = [self.__y_train[i] for i in nearest[:self.k]]
        # 分类投票
        votes = Counter(topK_y)
        # 得出预测结果
        predict_y = votes.most_common(1)[0][0]
        return predict_y
    def __str__(self):
        return "k=%d"%(self.k)
    def __repr__(self):
        return "k=%d"%(self.k)
```

### 训练数据集和测试数据集分离

#### 使用sklearn中的train_test_split

```python
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=666)
print(X_train.shape)
```

#### 模拟封装sklearn中的train_test_split

```python
import numpy as np
def train_test_split(X,y,test_radio = 0.2,seed = None):
    assert X.shape[0] == y.shape[0],\
    "the size of X must be equal to the size of y"
    assert 0<=test_radio<=1,\
    "test_radio must be valid"
    if seed:
        np.random.seed(seed)

    shuffled_indexes = np.random.permutation(len(X))
    test_size = int(len(X) * test_radio)
    test_indexes = shuffled_indexes[:test_size]
    train_indexes = shuffled_indexes[test_size:]
    X_test = X[test_indexes]
    y_test = y[test_indexes]
    X_train = X[train_indexes]
    y_train = y[train_indexes]
    return X_train,X_test,y_train,y_test
```

### 分类准确度

#### 使用sklearn中的accuracy_score

```python
import numpy as np
from sklearn import datasets
digits = datasets.load_digits()
X = digits.data
y = digits.target
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=666)
from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier(n_neighbors=6)
knn_classifier.fit(X_train,y_train)
y_predict = knn_classifier.predict(X_test)
#sklearn中的accuracy_score
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_predict)
```

#### 模拟封装accuracy_score

```python
def accuracy_score(y_true,y_predict):
    """计算y_true和y_predict之间的准确率"""
    assert y_true.shape[0] == y_predict.shape[0],\
    "the size of y_true must be equal to the size of y_predict"

    return sum(y_true == y_predict) / len(y_predict)
```

### 超参数和模型参数

![](Snipaste_2020-11-12_16-32-24.png)

#### 在knn算法中寻找最好的超参数K值

```python
best_score = 0
best_k = -1
for k in range(1,11):
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(X_train,y_train)
    score = knn_classifier.score(X_test,y_test)
    if score > best_score:
        best_score = score
        best_k = k
print("best_k",best_k)
print("best_score",best_score)
```

#### knn算法默认不考虑距离的权重，但有些情况下考虑权重是相对比较合适的，我们可以通过设置weights属性决定knn算法是否考虑距离权重。我们可以以距离的倒数作为权重。

#### 算法实现（自己思考的）：先选出k个距离最近的（可得到他们的索引和距离）,然后需要统计他们所属的类别（可以轻易得出，可以采取遍历放在dict的策略），然后根据类别分别计算出权重和，得出最后答案。

#### knn算法中weights也是一个超参数

```python
best_method = ""
best_score = 0
best_k = -1
for method in ["uniform","distance"]:
    for k in range(1,11):
        knn_classifier = KNeighborsClassifier(n_neighbors=k,weights=method)
        knn_classifier.fit(X_train,y_train)
        score = knn_classifier.score(X_test,y_test)
        if score > best_score:
            best_method = method
            best_score = score
            best_k = k
print("best_k",best_k)
print("best_score",best_score)
print("best_method",best_method)
```

#### sklearn中knn算法使用的是明科夫斯基距离，默认取2，即为欧拉距离，对于sklearn中我们还可以对明科夫斯基距离中的p这一超参数进行搜索

```python
best_p = -1
best_score = 0
best_k = -1
for p in range(1,6):
    for k in range(1,11):
        knn_classifier = KNeighborsClassifier(n_neighbors=k,p=p)
        knn_classifier.fit(X_train,y_train)
        score = knn_classifier.score(X_test,y_test)
        if score > best_score:
            best_p = p
            best_score = score
            best_k = k
print("best_k",best_k)
print("best_score",best_score)
print("best_p",best_p)
```

### 网格化搜索确定最佳超参数

```python
import numpy as np
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

digits = datasets.load_digits()
X = digits.data
y = digits.target
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 666)
knn_classifier = KNeighborsClassifier()
param_grid = [
    {
        'weights': ['uniform'], 
        'n_neighbors': [i for i in range(1, 11)]
    },
    {
        'weights': ['distance'],
        'n_neighbors': [i for i in range(1, 11)], 
        'p': [i for i in range(1, 6)]
    }
]
grid_search = GridSearchCV(knn_classifier,param_grid)
grid_search.fit(X_train,y_train)

knn_classifier = grid_search.best_estimator_
knn_classifier.score(X_test,y_test)
```

### knn算法默认使用明科夫斯基距离，可以通过改变metrics参数改用其他距离计算公式

### 数据归一化：最好使用均值方差归一化

#### 防止距离被某一特征所主导，数据归一化就是将所有的数据映射到统一尺度

![](Snipaste_2020-11-12_22-08-37.png)

![](Snipaste_2020-11-12_22-10-48.png)

### 均值方差归一化比最值归一化好在哪个地方？

#### 最值归一化：缺点是抗干扰能力弱，受离群值影响比较大，中间容易没有数据。最大最小值归一化后的数据落在[0,1]之间。假设某个特征下有一组数据：1,2,3,4,5,100那么对数据使用最大最小值归一化后的值为：0,2/99,3/99,4/99,1。中间没有数据，受离群值100的影响大。适用于有明显边界的情况，比如说成绩

####方差归一化抗干扰能力强，和所有数据有关，求标准差需要所有的值介入，若有离群值，会被抑制下来。但是归一化后的数据最终的结果不一定落在0到1之间。几乎适用于所有情况

#### 最值归一化

```python
import numpy as np
x = np.random.randint(0,100,100)
x = np.array(x,dtype=float)
#对一维数组进行最值归一化
(x - np.min(x)) / (np.max(x) - np.min(x))

X = x.reshape(50,2)
#对二维数组进行最值归一化
X[:,0] = (X[:,0] - np.min(X[:,0])) / (np.max(X[:,0]) - np.min(X[:,0]))
X[:,1] = (X[:,1] - np.min(X[:,1])) / (np.max(X[:,1]) - np.min(X[:,1]))
# X
print(np.mean(X[:,0]))
print(np.std(X[:,0]))
```

#### 均值方差归一化

```python
import numpy as np
x = np.random.randint(0,100,100)
x = np.array(x,dtype=float)
#对一维数组进行均值方差归一化
# x = (x - np.mean(x)) / np.std(x)
# print(x)
# print(x.mean())
# print(x.std())
X = x.reshape(50,2)
#对二维数组进行均值方差归一化
X[:,0] = (X[:,0] - np.mean(X[:,0])) / np.std(X[:,0])
X[:,1] = (X[:,1] - np.mean(X[:,1])) / np.std(X[:,1])
# X
print(np.mean(X[:,0]))
print(np.std(X[:,0]))

print(np.mean(X[:,1]))
print(np.std(X[:,1]))
```

### 注意点:测试数据集的归一化要使用训练数据集的均值和方差，因为真实环境下的所有数据均值和方差是不知道的；

![](Snipaste_2020-11-13_10-04-26.png)

![](Snipaste_2020-11-13_10-08-46.png)

### 使用scikit-learn中的Scaler对数据进行归一化

```python
import numpy as np
from sklearn import datasets
iris = datasets.load_iris()
X = iris.data
y = iris.target
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=666)
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()
standardScaler.fit(X_train)
#fit之后standardScaler中保存了这组数据的均值、方差等信息
print(standardScaler.mean_)
print(standardScaler.scale_)
#使用transform真正实现对数据的归一化
X_train = standardScaler.transform(X_train)
X_test = standardScaler.transform(X_test)

print(np.mean(X_train[:,0]))
print(np.std(X_train[:,0]))
print(np.mean(X_test[:,0]))
print(np.std(X_test[:,0]))
```

#### 模拟StandardScaler实现

```python
import numpy as np
class StandardScaler:
    def __init__(self):
        self.mean_ = None
        self.scale_ = None
        self.column_ = None
    def fit(self,X_train):
        assert X_train.ndim == 2,"the dimension of X_train must be 2"
        """根据传来的X_trian数据集得到均值和方差"""
        self.column_ = X_train.shape[1]
        self.mean_ = np.array([np.mean(X_train[:,col])for col in range(self.column_)])
        self.scale_ = np.array([np.std(X_train[:,col]) for col in range(self.column_)])
        return self
    def transform(self,X):
        assert X.ndim == 2, "the dimension of X_train must be 2"
        assert self.mean_ is not None and self.scale_ is not None and self.column_ is not None,\
        "must fit before transform"
        assert self.column_ == X.shape[1],\
        "the feature number of X must be equal the fit's X_train"
        resX = np.empty(shape=X.shape,dtype=float)
        for col in range(self.column_):
            resX[:,col] = (X[:,col] - self.mean_[col]) / self.scale_[col]
        return resX
    def __repr__(self):
        return "自定义StandardScaler"
```

### 总结：

#### (1)K近邻算法是一个天然解决分类问题的算法，天然解决多分类问题

#### (2)K近邻算法也可以解决回归问题（KNeighborsRegressor类）

### K近邻算法的缺点

#### （1）K近邻算法效率低下

#### （2）高度数据相关：如果k取3，如果周围有两个数据错误而就预测错了

#### （3）预测结果具有不可解释性

#### （4）维数灾难：随着维数的增加，看似很近的两个点距离越来越远

![](Snipaste_2020-11-13_14-41-52.png)

![](Snipaste_2020-11-13_14-44-11.png)









